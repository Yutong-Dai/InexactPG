{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "703517ed9bdc81570f26788304b686af16b08fb69c73a3571779325daddaa49b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.append(\"../\")\n",
    "from src.params import *\n",
    "import src.utils as utils\n",
    "from src.lossfunction import LogisticLoss\n",
    "from src.regularizer import GL1\n",
    "from src.naive.ProbGL1 import ProbGL1\n",
    "from numba import jit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Problem import Problem\n",
    "@jit(nopython=True, cache=True)\n",
    "def _proximal_gradient_jit(X, alpha, gradf, K, p, starts, ends, Lambda_group):\n",
    "    proximal = np.zeros((p, 1))\n",
    "    for i in range(K):\n",
    "        start, end = starts[i], ends[i]\n",
    "        XG_i = X[start:end]\n",
    "        gradfG_i = gradf[start:end]\n",
    "        gradient_step = XG_i - alpha * gradfG_i\n",
    "        gradient_step_norm = np.sqrt(np.dot(gradient_step.T, gradient_step))[0][0]\n",
    "        if gradient_step_norm != 0:\n",
    "            temp = 1 - ((Lambda_group[i] * alpha) / gradient_step_norm)\n",
    "        else:\n",
    "            temp = -1\n",
    "        sG_i = max(temp, 0) * gradient_step\n",
    "        proximal[start:end] = sG_i\n",
    "    return proximal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbGL1(Problem):\n",
    "    def __init__(self, f, r) -> None:\n",
    "        super().__init__(f, r)\n",
    "        self.K = self.r.K\n",
    "        self.n, self.p = self.f.n, self.f.p\n",
    "        self.starts = self.r.starts\n",
    "        self.ends = self.r.ends\n",
    "        self.Lambda_group = self.r.Lambda_group\n",
    "\n",
    "    def func(self, x):\n",
    "        return self.f.evaluate_function_value(x) + self.r.evaluate_function_value_jit(x)\n",
    "\n",
    "    def gradf(self, x):\n",
    "        return self.f.gradient()\n",
    "\n",
    "    def ipg(self, xk, gradfxk, alphak, rxk, method, **kwargs):\n",
    "        if method == 'sampling':\n",
    "            init_epsilon = kwargs['init_epsilon']\n",
    "            t = kwargs['t']\n",
    "            mode = kwargs['mode']\n",
    "            seed = kwargs['seed']\n",
    "            xprox = self._pg(xk, gradfxk, alphak)\n",
    "            x = self._ipg_sample(xprox, gradfxk, rxk, alphak, init_epsilon, t, mode, seed)\n",
    "        elif method == 'algorithm':\n",
    "            raise ValueError(f'{method} is not implemented.')\n",
    "        else:\n",
    "            raise ValueError(f'{method} is not defined.')\n",
    "        return x\n",
    "\n",
    "    def _ipg_sample(self, xprox, xk, gradfxk, alphak, init_epsilon, t=1, mode='whole', seed=None, **kwargs):\n",
    "        self.seed = seed\n",
    "        epsilon = init_epsilon\n",
    "        ck = (np.sqrt(6 / (1 + t) * alphak) - np.sqrt(2 / alphak)) ** 2 / 4\n",
    "        count = 1\n",
    "        while True:\n",
    "            if count > 100:\n",
    "                raise utils.AlgorithmError(\"_ipg_sample: cannot sample a (x,y) pair satisfying the gap condition!\")\n",
    "            x, y = self._sample_primal_dual(xprox, xk, gradfxk, alphak, epsilon, mode)\n",
    "            diff = x - xk\n",
    "            gap = self._duality_gap(x, y, xk, gradfxk, alphak)\n",
    "            # print(f\"gap:{gap} | target:{np.dot(diff.T, diff)[0][0]} | ck:{ck}\")\n",
    "            if gap <= ck * (np.dot(diff.T, diff)[0][0]):\n",
    "                self.count = count\n",
    "                self.epsilon = epsilon\n",
    "                print(count, epsilon)\n",
    "                return x\n",
    "            epsilon *= 0.8\n",
    "            count += 1\n",
    "\n",
    "    def _sample_primal_dual(self, xprox, xk, gradfxk, alphak, epsilon, mode='whole', **kwargs):\n",
    "        if self.seed:\n",
    "            np.random.seed(self.seed)\n",
    "        if mode == 'whole':\n",
    "            delta = np.random.randn(*xprox.shape)\n",
    "            delta_norm = utils.l2_norm(delta)\n",
    "            delta *= (epsilon / delta_norm)\n",
    "            x = xprox + delta\n",
    "        elif mode == 'blocks':\n",
    "            raise utils.AlgorithmError(f\"mode:{mode} is not implemented yet.\")\n",
    "        else:\n",
    "            raise utils.AlgorithmError(f\"mode:{mode} is not defined.\")\n",
    "        gradient_step = xk - alphak * gradfxk\n",
    "        temp = (x - gradient_step) / alphak\n",
    "        dual_norm = self.r.dual(temp)\n",
    "        y = min(1, 1 / dual_norm) * temp\n",
    "        return x, y\n",
    "\n",
    "    def _duality_gap(self, x, y, xk, gradfxk, alphak):\n",
    "        gradient_step = xk - alphak * gradfxk\n",
    "        temp = x - gradient_step\n",
    "        primal = np.dot(temp.T, temp)[0][0] / (2 * alphak) + self.r.evaluate_function_value_jit(x)\n",
    "        dual_negative = ((alphak / 2) * (np.dot(y.T, y)) + np.dot(gradient_step.T, y))[0][0]\n",
    "        return primal + dual_negative\n",
    "\n",
    "    def _pg(self, xk, gradfxk, alphak):\n",
    "        xprox = _proximal_gradient_jit(xk, alphak, gradfxk,\n",
    "                                       self.K, self.p, self.starts,\n",
    "                                       self.ends, self.Lambda_group)\n",
    "        return xprox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Working on: a9a...\n"
     ]
    }
   ],
   "source": [
    "datasetName = 'a9a'\n",
    "frac = 0.4\n",
    "fileType = fileTypeDict[datasetName]\n",
    "print(\"Working on: {}...\".format(datasetName))\n",
    "X, y = utils.set_up_xy(datasetName, fileType, dbDir='../../db')\n",
    "f = LogisticLoss(X, y, datasetName)\n",
    "p = X.shape[1]\n",
    "num_of_groups = max(int(p * frac), 1)\n",
    "group = utils.gen_group(p, num_of_groups)\n",
    "Lambda = 1\n",
    "r = GL1(Lambda=Lambda, group=group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = ProbGL1(f, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "21 0.0011529215046068484\n"
     ]
    }
   ],
   "source": [
    "xk = np.zeros((f.p, 1))\n",
    "fxk = prob.func(xk)\n",
    "gradfxk = prob.gradf(xk)\n",
    "alphak = 1\n",
    "rxk = 1\n",
    "method='sampling';init_epsilon=0; t=1; mode='whole';seed=0\n",
    "xnew = prob.ipg(xk, gradfxk, alphak, rxk, method='sampling',init_epsilon=0.1, t=1, mode='whole', seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "xk = np.ones((f.p, 1))\n",
    "fxk = prob.func(xk)\n",
    "gradfxk = prob.gradf(xk)\n",
    "alphak = 1\n",
    "rxk = 1\n",
    "prob.seed = seed\n",
    "xprox = prob._pg(xk, gradfxk, alphak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = prob._sample_primal_dual(xprox, xk, gradfxk, alphak, epsilon=1e-5, mode='whole')\n",
    "prob._duality_gap(x, y, xk, gradfxk, alphak)\n",
    "np.sum(np.abs(x - xprox))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    def __init__(self, prob, params, **kwargs):\n",
    "        self.prob = prob\n",
    "        self.__dict__.update(params)\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'eta': 233, 'gamma':334}\n",
    "solver = Solver('prob', params, warm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(233, 334, 'prob')"
      ]
     },
     "metadata": {},
     "execution_count": 213
    }
   ],
   "source": [
    "solver.eta, solver.gamma, solver.prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'prob': 'prob', 'eta': 233, 'gamma': 334, 'warm': True}"
      ]
     },
     "metadata": {},
     "execution_count": 214
    }
   ],
   "source": [
    "solver.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}